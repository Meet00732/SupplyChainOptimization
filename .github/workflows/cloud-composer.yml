name: Deploy to Compute Engine & Cloud Composer

on:
  push:
    branches: testing-pipeline
  workflow_dispatch:
    inputs:
      deployment_target:
        description: 'Where to deploy Airflow'
        required: true
        default: 'compute_engine'
        type: choice 
        options:
          - local
          - cloud_composer
          - compute_engine
          - both

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r Data_Pipeline/requirements-test.txt
      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$(pwd)/Data_Pipeline:$(pwd)" >> $GITHUB_ENV
          echo "PYTHONPATH set to $(pwd)/Data_Pipeline:$(pwd)"
      - name: Run Unit Tests
        run: |
          python -m unittest discover -s Data_Pipeline/tests -p "test*.py"
      - name: Authenticate with Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: Set up Google Cloud SDK
        uses: google-github-actions/setup-gcloud@v1

      - name: Update Compute Engine VM Access Scopes
        run: |
          echo "üöÄ Stopping airflow-server..."
          gcloud compute instances stop airflow-server --zone us-central1-a

          echo "üöÄ Updating VM service account and access scopes..."
          gcloud compute instances set-service-account airflow-server \
            --zone us-central1-a \
            --service-account cloudcomputefordocker@primordial-veld-450618-n4.iam.gserviceaccount.com \
            --scopes cloud-platform

          echo "üöÄ Restarting airflow-server..."
          gcloud compute instances start airflow-server --zone us-central1-a

          echo "‚úÖ VM access scopes updated successfully!"

      # ‚úÖ **Ensure SSH Access after VM Restart**
      - name: Sync Airflow Project Files to Compute Engine
        run: |
          echo "üöÄ Syncing project files to airflow-server..."
          
          # Ensure SSH is accessible
          EXTERNAL_IP=$(gcloud compute instances describe airflow-server --zone us-central1-a --format="get(networkInterfaces[0].accessConfigs[0].natIP)")
          
          echo "üåç Checking SSH access on $EXTERNAL_IP..."
          for i in {1..10}; do
            nc -zv -w5 $EXTERNAL_IP 22 && break
            echo "‚è≥ SSH not available yet. Retrying in 10 seconds..."
            sleep 10
          done
          echo "‚úÖ SSH is now available!"

          # Ensure SSH connectivity is working
          ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ~/.ssh/github-actions-key \
            ${{ secrets.COMPUTE_ENGINE_USER }}@${EXTERNAL_IP} "echo '‚úÖ SSH connection successful'"

          # Sync project files
          rsync -avz -e "ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ~/.ssh/github-actions-key" \
            --exclude '.git' . ${{ secrets.COMPUTE_ENGINE_USER }}@${EXTERNAL_IP}:/opt/airflow

          echo "‚úÖ Files synced successfully."


      # ‚úÖ Deploy to Compute Engine (Since airflow-server already exists)
      - name: Deploy to Compute Engine
        if: ${{ github.event_name == 'push' || inputs.deployment_target == 'compute_engine' || inputs.deployment_target == 'both' }}
        run: |
          echo "üöÄ Connecting to airflow-server instance..."
          # ‚úÖ Ensure SSH Key Exists
          if [ ! -f ~/.ssh/github-actions-key ]; then
            ssh-keygen -t rsa -b 4096 -C "github-actions" -N "" -f ~/.ssh/github-actions-key
            echo "‚úÖ SSH Key generated!"
          else
            echo "‚úÖ SSH Key already exists!"
          fi
          # ‚úÖ Ensure SSH Key is Added to Compute Engine
          PUBLIC_KEY=$(cat ~/.ssh/github-actions-key.pub)
          EXISTING_KEYS=$(gcloud compute instances describe airflow-server --zone us-central1-a --format="value(metadata.ssh-keys)" || echo "")
          if [[ "$EXISTING_KEYS" != *"$PUBLIC_KEY"* ]]; then
            gcloud compute instances add-metadata airflow-server --zone us-central1-a --metadata=ssh-keys="${{ secrets.COMPUTE_ENGINE_USER }}:$PUBLIC_KEY"
            echo "‚úÖ SSH Key added to Compute Engine!"
          else
            echo "‚úÖ SSH Key already exists in Compute Engine!"
          fi
          # ‚úÖ Ensure Compute Engine is Running
          INSTANCE_STATUS=$(gcloud compute instances describe airflow-server --zone us-central1-a --format="value(status)" || echo "STOPPED")
          if [[ "$INSTANCE_STATUS" != "RUNNING" ]]; then
            echo "üöÄ Starting Compute Engine instance..."
            gcloud compute instances start airflow-server --zone us-central1-a
            echo "‚úÖ Compute Engine started."
          else
            echo "‚úÖ Compute Engine is already running."
          fi
      - name: Sync Airflow Project Files to Compute Engine
        run: |
          echo "üöÄ Syncing project files to airflow-server..."
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/github-actions-key ${{ secrets.COMPUTE_ENGINE_USER }}@${{ secrets.COMPUTE_ENGINE_IP }} << 'EOF'
          sudo mkdir -p /opt/airflow
          sudo chown -R $USER:$USER /opt/airflow
          sudo chmod -R 775 /opt/airflow
          EOF
          rsync -avz -e "ssh -o StrictHostKeyChecking=no -i ~/.ssh/github-actions-key" --exclude '.git' . ${{ secrets.COMPUTE_ENGINE_USER }}@${{ secrets.COMPUTE_ENGINE_IP }}:/opt/airflow
          echo "‚úÖ Files synced successfully."
      - name: Deploy Airflow on Compute Engine
        run: |
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/github-actions-key ${{ secrets.COMPUTE_ENGINE_USER }}@${{ secrets.COMPUTE_ENGINE_IP }} << 'EOF'
            echo "üöÄ Ensuring Docker is installed..."
            if ! command -v docker &> /dev/null; then
              echo "‚ùå Docker is not installed. Installing..."
              sudo apt-get update -y
              sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
            else
              echo "‚úÖ Docker is already installed."
            fi
            echo "üöÄ Ensuring Docker Compose is installed..."
            if ! command -v docker-compose &> /dev/null; then
              echo "‚ùå Docker Compose not found. Installing latest version..."
              DOCKER_COMPOSE_VERSION=$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep -Po '"tag_name": "\K.*?(?=")')
              sudo curl -L "https://github.com/docker/compose/releases/download/${DOCKER_COMPOSE_VERSION}/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
              sudo chmod +x /usr/local/bin/docker-compose
              sudo ln -sf /usr/local/bin/docker-compose /usr/bin/docker-compose
            else
              echo "‚úÖ Docker Compose is already installed."
            fi
            # ‚úÖ Ensure user has permission to run Docker
            echo "üîÑ Adding user to Docker group..."
            sudo usermod -aG docker $USER
            newgrp docker
            sudo systemctl restart docker
            mkdir -p /opt/airflow
            echo "airflow dir created."
            echo "üöÄ Ensuring GCP Key File exists..."
            if [ -d /opt/airflow/gcp-key.json ]; then
                echo "‚ö†Ô∏è Found directory at /opt/airflow/gcp-key.json. Removing it..."
                sudo rm -rf /opt/airflow/gcp-key.json
            fi
            echo "üöÄ Creating GCP Key File..."
            echo '${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}' | jq . > /opt/airflow/gcp-key.json
            chmod 644 /opt/airflow/gcp-key.json
            sudo chown ubuntu:docker /opt/airflow/gcp-key.json
            echo "‚úÖ GCP Key File Created."
            echo "üöÄ Fixing Airflow log directory permissions..."
            sudo mkdir -p /opt/airflow/logs
            sudo chmod -R 777 /opt/airflow/logs
            sudo chown -R $USER:$USER /opt/airflow/logs
            echo "‚úÖ Log directory permissions fixed."
            echo "‚úÖ Airflow successfully started!"
          EOF
      - name: Get Airflow Webserver IP
        run: |
          EXTERNAL_IP=$(gcloud compute instances describe airflow-server --zone us-central1-a --format="get(networkInterfaces[0].accessConfigs[0].natIP)")
          echo "üåç Airflow UI is available at: http://$EXTERNAL_IP:8080"
      # ‚úÖ Deploy to Cloud Composer Only if Selected
      - name: Deploy DAGs to Cloud Composer
        if: ${{ inputs.deployment_target == 'cloud_composer' || inputs.deployment_target == 'both' }}
        run: |
          echo "üöÄ Uploading DAGs to Cloud Composer..."
          gcloud storage cp -r Data_Pipeline/dags gs://your-cloud-composer-bucket/dags
          echo "‚úÖ DAGs uploaded successfully!"
      # ‚úÖ Remove SSH Key after deployment (Security)
      - name: Remove SSH Key
        run: rm -f ~/.ssh/github-actions-key ~/.ssh/github-actions-key.pub