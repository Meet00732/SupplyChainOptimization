name: Deploy to Compute Engine & Cloud Composer

on:
  push:
    branches: testing-pipeline
  workflow_dispatch:
    inputs:
      deployment_target:
        description: 'Where to deploy Airflow'
        required: true
        default: 'compute_engine'
        type: choice 
        options:
          - local
          - cloud_composer
          - compute_engine
          - both

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r Data_Pipeline/requirements-test.txt
      - name: Set PYTHONPATH
        run: |
          echo "PYTHONPATH=$(pwd)/Data_Pipeline:$(pwd)" >> $GITHUB_ENV
          echo "PYTHONPATH set to $(pwd)/Data_Pipeline:$(pwd)"
      - name: Run Unit Tests
        run: |
          python -m unittest discover -s Data_Pipeline/tests -p "test*.py"

      - name: Authenticate with Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: Set Up gcloud CLI
        uses: google-github-actions/setup-gcloud@v1

      # üöÄ Create Compute Engine Instance If It Does Not Exist
      - name: Check and Create Compute Engine Instance
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_ZONE: "us-central1-a"
          INSTANCE_NAME: "airflow-server"
          MACHINE_TYPE: "e2-standard-2"
          IMAGE_FAMILY: "debian-11"
          IMAGE_PROJECT: "debian-cloud"
        run: |
          INSTANCE_EXISTS=$(gcloud compute instances list --filter="name=($INSTANCE_NAME)" --format="value(name)")
          
          if [ -z "$INSTANCE_EXISTS" ]; then
            echo "üöÄ Creating new Compute Engine instance..."
            gcloud compute instances create $INSTANCE_NAME \
              --project=$GCP_PROJECT_ID \
              --zone=$GCP_ZONE \
              --machine-type=$MACHINE_TYPE \
              --image-family=$IMAGE_FAMILY \
              --image-project=$IMAGE_PROJECT \
              --boot-disk-size=50GB \
              --tags=http-server,https-server
            echo "‚úÖ Compute Engine instance created successfully."
          else
            echo "‚úÖ Compute Engine instance already exists."
          fi


      # üöÄ Get Compute Engine External IP
      - name: Get Compute Engine External IP
        id: get-ip
        run: |
          EXTERNAL_IP=$(gcloud compute instances describe airflow-server \
            --zone us-central1-a --format="value(networkInterfaces[0].accessConfigs[0].natIP)")
          echo "COMPUTE_ENGINE_IP=$EXTERNAL_IP" >> $GITHUB_ENV
          echo "üåç Compute Engine IP: $EXTERNAL_IP"

      # üöÄ Generate SSH Key for Compute Engine Access
      - name: Generate SSH Key for GitHub Actions
        run: |
          ssh-keygen -t rsa -b 4096 -C "github-actions" -N "" -f ~/.ssh/github-actions-key
          echo "COMPUTE_ENGINE_SSH_KEY=$(cat ~/.ssh/github-actions-key)" >> $GITHUB_ENV
          echo "‚úÖ SSH Key generated."

      # üöÄ Add SSH Key to Compute Engine
      - name: Add SSH Key to Compute Engine
        run: |
          SSH_PUBLIC_KEY=$(cat ~/.ssh/github-actions-key.pub)
          gcloud compute instances add-metadata airflow-server \
            --zone us-central1-a \
            --metadata "ssh-keys=ubuntu:$SSH_PUBLIC_KEY"

      # üöÄ Upload Docker Compose Files to Compute Engine
      - name: Copy Docker Files to Compute Engine
        uses: appleboy/scp-action@v0.1.4
        with:
          host: ${{ env.COMPUTE_ENGINE_IP }}
          username: "ubuntu"
          key: ${{ env.COMPUTE_ENGINE_SSH_KEY }}
          source: "docker-compose.yaml, .env, dags"
          target: "~/airflow-deployment"

      # üöÄ Deploy Airflow via SSH and Run Docker-Compose
      - name: Run `docker-compose up` on Compute Engine
        uses: appleboy/ssh-action@master
        with:
          host: ${{ env.COMPUTE_ENGINE_IP }}
          username: "ubuntu"
          key: ${{ env.COMPUTE_ENGINE_SSH_KEY }}
          script: |
            sudo apt update && sudo apt install -y docker.io docker-compose
            mkdir -p ~/airflow-deployment
            cd ~/airflow-deployment
            docker-compose down
            docker-compose up -d --build
            echo "‚úÖ Airflow deployed successfully!"

      - name: Show Airflow UI URL
        run: |
          echo "üåç Airflow UI: http://${{ env.COMPUTE_ENGINE_IP }}:8080"

      # ‚úÖ Deploy DAGs to Cloud Composer (Optional)
      - name: Deploy DAGs to Cloud Composer
        if: ${{ github.event.inputs.deployment_target == 'cloud_composer' || github.event.inputs.deployment_target == 'both' }}
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_COMPOSER_ENVIRONMENT: ${{ secrets.GCP_COMPOSER_ENVIRONMENT }}
          GCP_COMPOSER_REGION: ${{ secrets.GCP_COMPOSER_REGION }}
        run: |
          DAGS_BUCKET=$(gcloud composer environments describe $GCP_COMPOSER_ENVIRONMENT \
            --location=$GCP_COMPOSER_REGION \
            --format="value(config.dagGcsPrefix)")
          echo "Uploading DAGs to $DAGS_BUCKET"
          gsutil -m cp -r dags/* $DAGS_BUCKET/

